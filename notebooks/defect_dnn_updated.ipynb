{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label Defect Prediction using Deep Neural Network\n",
    "\n",
    "This notebook implements a Deep Neural Network (DNN) model using PyTorch for multi-label defect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import hamming_loss, classification_report, f1_score, precision_score, recall_score\n",
    "import joblib\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.defect_utils import preprocess_data, get_defect_types\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectPredictionDNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size1=128, hidden_size2=64, dropout_rate=0.3):\n",
    "        super(DefectPredictionDNN, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size2, output_size),\n",
    "            nn.Sigmoid()  # Sigmoid for multi-label classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/dataset.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values:\")\n",
    "print(missing_values[missing_values > 0] if any(missing_values > 0) else \"No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the defect labels\n",
    "defect_counts = {}\n",
    "for defects in df['defects']:\n",
    "    for defect in defects.split(','):\n",
    "        defect_counts[defect] = defect_counts.get(defect, 0) + 1\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "defect_df = pd.DataFrame(list(defect_counts.items()), columns=['Defect', 'Count'])\n",
    "defect_df = defect_df.sort_values('Count', ascending=False)\n",
    "\n",
    "# Plot defect distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Defect', y='Count', data=defect_df)\n",
    "plt.title('Defect Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "print(\"Defect distribution:\")\n",
    "for defect, count in defect_counts.items():\n",
    "    print(f\"{defect}: {count} ({count/len(df):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "# Extract features (all columns except 'defects')\n",
    "feature_cols = [col for col in df.columns if col != 'defects']\n",
    "X = df[feature_cols].values\n",
    "\n",
    "# Process labels\n",
    "defects = df['defects'].str.split(',').tolist()\n",
    "\n",
    "# Convert to multi-hot encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(defects)\n",
    "\n",
    "# Get the defect class names\n",
    "defect_classes = mlb.classes_\n",
    "print(f\"Defect classes: {defect_classes}\")\n",
    "print(f\"Number of classes: {len(defect_classes)}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"Label matrix shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare PyTorch Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train the DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "model = DefectPredictionDNN(input_size=input_size, output_size=output_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Convert outputs to binary predictions\n",
    "            preds = (outputs > 0.5).float()\n",
    "            \n",
    "            # Store predictions and labels for metric calculation\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    epoch_val_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_val_f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "    \n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_f1_scores.append(epoch_val_f1)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f} - \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f} - \"\n",
    "          f\"Val F1: {epoch_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs+1), val_f1_scores, label='Validation F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Validation F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Convert outputs to binary predictions\n",
    "        preds = (outputs > 0.5).float()\n",
    "        \n",
    "        # Store predictions, raw outputs, and labels\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_outputs.append(outputs.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_outputs = np.vstack(all_outputs)\n",
    "all_labels = np.vstack(all_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "hamming = hamming_loss(all_labels, all_preds)\n",
    "micro_f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "micro_precision = precision_score(all_labels, all_preds, average='micro')\n",
    "macro_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "micro_recall = recall_score(all_labels, all_preds, average='micro')\n",
    "macro_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Test Metrics:\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"Micro-F1 Score: {micro_f1:.4f}\")\n",
    "print(f\"Macro-F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"Micro-Precision: {micro_precision:.4f}\")\n",
    "print(f\"Macro-Precision: {macro_precision:.4f}\")\n",
    "print(f\"Micro-Recall: {micro_recall:.4f}\")\n",
    "print(f\"Macro-Recall: {macro_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "class_report = classification_report(all_labels, all_preds, target_names=defect_classes, output_dict=True)\n",
    "class_metrics = pd.DataFrame(class_report).transpose()\n",
    "class_metrics = class_metrics.drop('accuracy', errors='ignore')\n",
    "\n",
    "# Display per-class metrics\n",
    "print(\"Per-class metrics:\")\n",
    "display(class_metrics)\n",
    "\n",
    "# Visualize per-class F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=class_metrics.index[:-3], y=class_metrics['f1-score'][:-3])\n",
    "plt.title('F1 Score per Defect Class')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Precision@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Precision@k\n",
    "def precision_at_k(y_true, y_score, k):\n",
    "    \"\"\"Calculate Precision@k for multi-label classification.\"\"\"\n",
    "    # Get the indices of the top k predictions for each sample\n",
    "    top_k_indices = np.argsort(y_score, axis=1)[:, ::-1][:, :k]\n",
    "    \n",
    "    # Create a matrix of predictions with 1s at the top k positions\n",
    "    y_pred_k = np.zeros_like(y_score)\n",
    "    for i, indices in enumerate(top_k_indices):\n",
    "        y_pred_k[i, indices] = 1\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if np.sum(y_pred_k[i]) > 0:  # Avoid division by zero\n",
    "            precision += np.sum(y_true[i] & y_pred_k[i]) / np.sum(y_pred_k[i])\n",
    "    \n",
    "    return precision / len(y_true)\n",
    "\n",
    "# Calculate Precision@k for different values of k\n",
    "k_values = [1, 2, 3]\n",
    "for k in k_values:\n",
    "    p_at_k = precision_at_k(all_labels, all_outputs, k)\n",
    "    print(f\"Precision@{k}: {p_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize predictions for a sample\n",
    "def visualize_predictions(sample_idx, y_true, y_pred, y_score, class_names):\n",
    "    \"\"\"Visualize predictions for a single sample.\"\"\"\n",
    "    true_labels = y_true[sample_idx]\n",
    "    pred_labels = y_pred[sample_idx]\n",
    "    pred_scores = y_score[sample_idx]\n",
    "    \n",
    "    # Create a DataFrame for visualization\n",
    "    df = pd.DataFrame({\n",
    "        'Class': class_names,\n",
    "        'True': true_labels,\n",
    "        'Predicted': pred_labels,\n",
    "        'Score': pred_scores\n",
    "    })\n",
    "    \n",
    "    # Sort by prediction score\n",
    "    df = df.sort_values('Score', ascending=False)\n",
    "    \n",
    "    # Create a color map\n",
    "    colors = []\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i]['True'] == 1 and df.iloc[i]['Predicted'] == 1:\n",
    "            colors.append('green')  # True positive\n",
    "        elif df.iloc[i]['True'] == 0 and df.iloc[i]['Predicted'] == 1:\n",
    "            colors.append('red')    # False positive\n",
    "        elif df.iloc[i]['True'] == 1 and df.iloc[i]['Predicted'] == 0:\n",
    "            colors.append('orange') # False negative\n",
    "        else:\n",
    "            colors.append('gray')   # True negative\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(df['Class'], df['Score'], color=colors)\n",
    "    plt.axvline(x=0.5, color='black', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel('Prediction Score')\n",
    "    plt.title(f'Defect Predictions for Sample {sample_idx}')\n",
    "    plt.xlim(0, 1)\n",
    "    \n",
    "    # Add a legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='green', label='True Positive'),\n",
    "        Patch(facecolor='red', label='False Positive'),\n",
    "        Patch(facecolor='orange', label='False Negative'),\n",
    "        Patch(facecolor='gray', label='True Negative')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Visualize predictions for a few samples\n",
    "for sample_idx in range(3):  # Visualize first 3 samples\n",
    "    print(f\"\\nSample {sample_idx}:\")\n",
    "    df = visualize_predictions(sample_idx, all_labels, all_preds, all_outputs, defect_classes)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, scaler, and label binarizer\n",
    "model_path = '../models/dnn_defect.pt'\n",
    "scaler_path = '../models/dnn_defect_scaler.pkl'\n",
    "mlb_path = '../models/dnn_defect_mlb.pkl'\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "joblib.dump(mlb, mlb_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "print(f\"MultiLabelBinarizer saved to {mlb_path}\")\n",
    "\n",
    "# Save model architecture information for later loading\n",
    "model_info = {\n",
    "    'input_size': input_size,\n",
    "    'output_size': output_size,\n",
    "    'hidden_size1': 128,\n",
    "    'hidden_size2': 64,\n",
    "    'dropout_rate': 0.3\n",
    "}\n",
    "joblib.dump(model_info, '../models/dnn_defect_info.pkl')\n",
    "print(f\"Model info saved to ../models/dnn_defect_info.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label Defect Prediction using Logistic Regression\n",
    "\n",
    "This notebook implements a Logistic Regression model for multi-label defect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, classification_report, f1_score, precision_score, recall_score\n",
    "import joblib\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.defect_utils import preprocess_data, get_defect_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/dataset.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values:\")\n",
    "print(missing_values[missing_values > 0] if any(missing_values > 0) else \"No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the defect labels\n",
    "defect_counts = {}\n",
    "for defects in df['defects']:\n",
    "    for defect in defects.split(','):\n",
    "        defect_counts[defect] = defect_counts.get(defect, 0) + 1\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "defect_df = pd.DataFrame(list(defect_counts.items()), columns=['Defect', 'Count'])\n",
    "defect_df = defect_df.sort_values('Count', ascending=False)\n",
    "\n",
    "# Plot defect distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Defect', y='Count', data=defect_df)\n",
    "plt.title('Defect Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "print(\"Defect distribution:\")\n",
    "for defect, count in defect_counts.items():\n",
    "    print(f\"{defect}: {count} ({count/len(df):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "# Extract features (all columns except 'defects')\n",
    "feature_cols = [col for col in df.columns if col != 'defects']\n",
    "X = df[feature_cols].values\n",
    "\n",
    "# Process labels\n",
    "defects = df['defects'].str.split(',').tolist()\n",
    "\n",
    "# Convert to multi-hot encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(defects)\n",
    "\n",
    "# Get the defect class names\n",
    "defect_classes = mlb.classes_\n",
    "print(f\"Defect classes: {defect_classes}\")\n",
    "print(f\"Number of classes: {len(defect_classes)}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"Label matrix shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Logistic Regression model with OneVsRestClassifier for multi-label classification\n",
    "base_lr = LogisticRegression(solver='liblinear', random_state=42)\n",
    "lr_model = OneVsRestClassifier(base_lr)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'estimator__C': [0.1, 1.0, 10.0],\n",
    "    'estimator__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    lr_model,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_micro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_lr_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "y_val_pred = best_lr_model.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "hamming = hamming_loss(y_val, y_val_pred)\n",
    "micro_f1 = f1_score(y_val, y_val_pred, average='micro')\n",
    "macro_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "micro_precision = precision_score(y_val, y_val_pred, average='micro')\n",
    "macro_precision = precision_score(y_val, y_val_pred, average='macro')\n",
    "micro_recall = recall_score(y_val, y_val_pred, average='micro')\n",
    "macro_recall = recall_score(y_val, y_val_pred, average='macro')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Validation Metrics:\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"Micro-F1 Score: {micro_f1:.4f}\")\n",
    "print(f\"Macro-F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"Micro-Precision: {micro_precision:.4f}\")\n",
    "print(f\"Macro-Precision: {macro_precision:.4f}\")\n",
    "print(f\"Micro-Recall: {micro_recall:.4f}\")\n",
    "print(f\"Macro-Recall: {macro_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Precision@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Precision@k\n",
    "def precision_at_k(y_true, y_score, k):\n",
    "    \"\"\"Calculate Precision@k for multi-label classification.\"\"\"\n",
    "    # Get the indices of the top k predictions for each sample\n",
    "    top_k_indices = np.argsort(y_score, axis=1)[:, ::-1][:, :k]\n",
    "    \n",
    "    # Create a matrix of predictions with 1s at the top k positions\n",
    "    y_pred_k = np.zeros_like(y_score)\n",
    "    for i, indices in enumerate(top_k_indices):\n",
    "        y_pred_k[i, indices] = 1\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if np.sum(y_pred_k[i]) > 0:  # Avoid division by zero\n",
    "            precision += np.sum(y_true[i] & y_pred_k[i]) / np.sum(y_pred_k[i])\n",
    "    \n",
    "    return precision / len(y_true)\n",
    "\n",
    "# Get probability scores for validation set\n",
    "y_val_score = best_lr_model.predict_proba(X_val)\n",
    "\n",
    "# Calculate Precision@k for different values of k\n",
    "k_values = [1, 2, 3]\n",
    "for k in k_values:\n",
    "    p_at_k = precision_at_k(y_val, y_val_score, k)\n",
    "    print(f\"Precision@{k}: {p_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_test_pred = best_lr_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "hamming = hamming_loss(y_test, y_test_pred)\n",
    "micro_f1 = f1_score(y_test, y_test_pred, average='micro')\n",
    "macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "micro_precision = precision_score(y_test, y_test_pred, average='micro')\n",
    "macro_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "micro_recall = recall_score(y_test, y_test_pred, average='micro')\n",
    "macro_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Test Metrics:\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"Micro-F1 Score: {micro_f1:.4f}\")\n",
    "print(f\"Macro-F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"Micro-Precision: {micro_precision:.4f}\")\n",
    "print(f\"Macro-Precision: {macro_precision:.4f}\")\n",
    "print(f\"Micro-Recall: {micro_recall:.4f}\")\n",
    "print(f\"Macro-Recall: {macro_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "class_report = classification_report(y_test, y_test_pred, target_names=defect_classes, output_dict=True)\n",
    "class_metrics = pd.DataFrame(class_report).transpose()\n",
    "class_metrics = class_metrics.drop('accuracy', errors='ignore')\n",
    "\n",
    "# Display per-class metrics\n",
    "print(\"Per-class metrics:\")\n",
    "display(class_metrics)\n",
    "\n",
    "# Visualize per-class F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=class_metrics.index[:-3], y=class_metrics['f1-score'][:-3])\n",
    "plt.title('F1 Score per Defect Class')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance (coefficients) for each class\n",
    "feature_names = feature_cols\n",
    "coefficients = []\n",
    "\n",
    "for i, defect_class in enumerate(defect_classes):\n",
    "    # Get the coefficients for this class\n",
    "    coef = best_lr_model.estimators_[i].coef_[0]\n",
    "    coefficients.append(coef)\n",
    "\n",
    "# Convert to DataFrame\n",
    "coef_df = pd.DataFrame(coefficients, index=defect_classes, columns=feature_names)\n",
    "\n",
    "# Plot heatmap of coefficients\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(coef_df, cmap='coolwarm', center=0, annot=False)\n",
    "plt.title('Logistic Regression Coefficients per Defect Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the top 3 most important features for each class\n",
    "print(\"Top 3 most important features for each defect class:\")\n",
    "for defect_class in defect_classes:\n",
    "    # Get the absolute coefficients for this class\n",
    "    abs_coef = np.abs(coef_df.loc[defect_class])\n",
    "    # Get the top 3 features\n",
    "    top_features = abs_coef.nlargest(3).index.tolist()\n",
    "    # Get the coefficient values\n",
    "    top_values = coef_df.loc[defect_class, top_features].tolist()\n",
    "    print(f\"{defect_class}:\")\n",
    "    for feature, value in zip(top_features, top_values):\n",
    "        print(f\"  {feature}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, scaler, and label binarizer\n",
    "model_path = '../models/logistic_defect.pkl'\n",
    "scaler_path = '../models/logistic_defect_scaler.pkl'\n",
    "mlb_path = '../models/logistic_defect_mlb.pkl'\n",
    "\n",
    "joblib.dump(best_lr_model, model_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "joblib.dump(mlb, mlb_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "print(f\"MultiLabelBinarizer saved to {mlb_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
